#version 450

#extension GL_EXT_debug_printf : enable
#extension GL_EXT_shader_8bit_storage : enable
#extension GL_ARB_gpu_shader_int64 : enable

layout(local_size_x = 64, local_size_y = 1, local_size_z = 1) in;

layout(set = 0, binding = 0) readonly buffer InputBuffer {
	uint in_data[];
};

layout(set = 0, binding = 1, r32ui) readonly uniform uimage2D table;

layout(set = 0, binding = 2) buffer OutputBuffer {
	uint out_data[];
};

layout(constant_id = 0) const uint max_pat_len = 0;

layout(push_constant) uniform ExtraInfo {
	uint64_t offset;
	uint input_len;
} extra_info;

// shared uint input_cache[64 / 4];

// ===

const uint64_t FNV_OFFSET_BASIS = 0xcbf29ce484222325ul;
const uint64_t FNV_PRIME = 0x100000001b3ul;

// ===

// Carry-less multiplication, simply discards the overflowing bits of the result
// Hopefully this loop is unrolled
uint64_t clmul(uint64_t x, uint64_t y) {
	uint64_t accum = 0;
	for(uint i = 0; i < 64; i++) {
		if((x & 1ul) == 1) {
			// In glsl, while the result of multiplication with overflow is undefined, addition with overflow simply yields the lower bits of the result
			accum += y;
		}
		x >>= 1ul;
		x ^= x & (1ul << 127);
		y <<= 1ul;
		y ^= y & 1ul;
	}

	return accum;
}

uint64_t match_id_hash_init() {
	return FNV_OFFSET_BASIS;
}

uint64_t match_id_hash_add(uint64_t hash, uint new_value) {
	return clmul((hash ^ uint64_t(new_value)), FNV_PRIME);
}

uint upper_of(uint64_t value) {
	return uint((value >> 32) & 0x00000000ffffffff);
}

uint lower_of(uint64_t value) {
	return uint(value & 0x00000000ffffffff);
}

// start_idx and end_idx are relative to the current in_data
void write_match(uint64_t id, uint64_t start_idx, uint64_t end_idx) {
	uint idx = atomicAdd(out_data[0], 1);
	idx *= 6; // idx * data size in uints
	idx += 1; // offset from length at start

	uint out_data_len = 1024 * 1024;
	// If we're overflowing, just don't write a match. Maybe return false, but there's much not point cause the shader can't really do anything about it
	if((idx + 5) >= out_data_len) {
		return;
	}

	out_data[idx] = lower_of(id);
	out_data[idx + 1] = upper_of(id);
	out_data[idx + 2] = lower_of(start_idx);
	out_data[idx + 3] = upper_of(start_idx);
	out_data[idx + 4] = lower_of(end_idx);
	out_data[idx + 5] = upper_of(end_idx);
}

// ===

void main() {
	if(gl_GlobalInvocationID.x > extra_info.input_len) {
		return;
	}

	// NOTE: Using shared memory has not been proven particularly useful for performance, and maybe even slows down performance. The bottleneck seems to be the memcpys, but I'm not sure why.
	//       Anyway, if I was to revisit using shared memory to cache input data, I will need to think about bank collisions and make sure to fix the case of the shader reading past the
	//       end of the shared memory in the case of a match at the end of a thread group

	// if((gl_LocalInvocationID.x & 3) == 0) {
	// 	uint cache_idx = gl_LocalInvocationID.x / 4;
	// 	uint input_idx = gl_GlobalInvocationID.x / 4;

	// 	// Reads from the buffer should be coalesced
	// 	uint dat = in_data[input_idx];
	// 	input_cache[cache_idx] = dat;
	// 	// debugPrintfEXT("input_cache[%u] = in_data[%u] (%u)\n", base_cache_idx, input_idx, dat);
	// }

	// Await caching of data
	// memoryBarrierShared();

	// if((uint64_t(gl_GlobalInvocationID.x) + extra_info.offset) == 724531255ul) {
	// 	debugPrintfEXT("(724531255ul) LocID %u\n", gl_LocalInvocationID.x);
	// 	for(uint i = 0; i <= max_pat_len; i++) {
	// 		uint idx = gl_LocalInvocationID.x + i;
	// 		uint cache_idx = idx / 4;
	// 		uint curr_val = (input_cache[cache_idx] >> ((idx & 3) * 8)) & 0xff;
	// 		debugPrintfEXT("\t[%u]: %u", i, curr_val);
	// 	}
	// }

	uint state = 0;
	uint64_t id = match_id_hash_init();

	for(uint i = 0; (i <= max_pat_len + 1) && ((i + gl_GlobalInvocationID.x) < extra_info.input_len); i++) {
		uint idx = gl_LocalInvocationID.x + i;
		uint gidx = gl_GlobalInvocationID.x + i;

		// uint cache_idx = idx / 4;
		uint input_idx = gidx / 4;
		// uint curr_val = (input_cache[cache_idx] >> ((idx & 3) * 8)) & 0xff;
		uint curr_val = (in_data[input_idx] >> ((gidx & 3) * 8)) & 0xff;

		// Lookup the next state in the STT
		uvec4 texel = imageLoad(table, ivec2(curr_val, state));
		uvec4 dot_texel = imageLoad(table, ivec2(256, state));
		bool choose_dot = texel.r == 0;
		uint next_state = uint(choose_dot) * dot_texel.r + uint(!choose_dot) * texel.r;

		if(next_state == 0xffffffff) {
			// debugPrintfEXT("LocID %u / GlobID %u: wrote match at LocIdx %u / GlobIdx %u, offset = %u\n", gl_LocalInvocationID.x, gl_GlobalInvocationID.x, idx, gl_GlobalInvocationID.x + i, extra_info.offset);
			uint64_t start_idx = uint64_t(gl_GlobalInvocationID.x + extra_info.offset);
			uint64_t end_idx = uint64_t(gl_GlobalInvocationID.x + i + extra_info.offset - 1);
			write_match(id, start_idx, end_idx);
			break;
		} else if(next_state != 0) {
			state = next_state;
			id = match_id_hash_add(id, uint(choose_dot) * 0x8000 + uint(!choose_dot) * curr_val);
			// debugPrintfEXT("Moving on to state = %u, matched at idx = %u\n", next_state, idx);
		} else {
			// debugPrintfEXT("Fail @ val = %u, state = %u, idx = %u, cache_idx = %u\n", curr_val, state, idx, cache_idx);
			break;
		}
	}
}